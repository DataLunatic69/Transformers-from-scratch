{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install BPEmb\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_network(d_model, hidden_dim):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "    super(EncoderBlock, self).__init__()\n",
    "\n",
    "    self.mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
    "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x, training, mask):\n",
    "    mhsa_output, attn_weights = self.mhsa(x, x, x, mask)\n",
    "    mhsa_output = self.dropout1(mhsa_output, training=training)\n",
    "    mhsa_output = self.layernorm1(x + mhsa_output)\n",
    "\n",
    "    ffn_output = self.ffn(mhsa_output)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    output = self.layernorm2(mhsa_output + ffn_output)\n",
    "\n",
    "    return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_block = EncoderBlock(12, 3, 48)\n",
    "\n",
    "block_output,  _ = encoder_block(x, True, None)\n",
    "print(f\"Output from single encoder block {block_output.shape}:\")\n",
    "print(block_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English tokenizer.\n",
    "bpemb_en = BPEmb(lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_vocab_size, bpemb_embed_size = bpemb_en.vectors.shape\n",
    "print(\"Vocabulary size:\", bpemb_vocab_size)\n",
    "print(\"Embedding size:\", bpemb_embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding for the word \"car\".\n",
    "bpemb_en.vectors[bpemb_en.words.index('car')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"Where can I find a pizzeria?\"\n",
    "tokens = bpemb_en.encode(sample_sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_seq = np.array(bpemb_en.encode_ids(\"Where can I find a pizzeria?\"))\n",
    "print(token_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embed = tf.keras.layers.Embedding(bpemb_vocab_size, embed_dim)\n",
    "token_embeddings = token_embed(token_seq)\n",
    "\n",
    "# The untrained embeddings for our sample sentence.\n",
    "print(\"Embeddings for: \", sample_sentence)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 256\n",
    "pos_embed = tf.keras.layers.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "# Generate ids for each position of the token sequence.\n",
    "pos_idx = tf.range(len(token_seq))\n",
    "print(pos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are our positon embeddings.\n",
    "position_embeddings = pos_embed(pos_idx)\n",
    "print(\"Position embeddings for the input sequence\\n\", position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = token_embeddings + position_embeddings\n",
    "print(\"Input to the initial encoder block:\\n\", input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, src_vocab_size,\n",
    "               max_seq_len, dropout_rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.max_seq_len = max_seq_len\n",
    "\n",
    "    self.token_embed = tf.keras.layers.Embedding(src_vocab_size, self.d_model)\n",
    "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
    "\n",
    "    # The original Attention Is All You Need paper applied dropout to the\n",
    "    # input before feeding it to the first encoder block.\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    # Create encoder blocks.\n",
    "    self.blocks = [EncoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) \n",
    "    for _ in range(num_blocks)]\n",
    "  \n",
    "  def call(self, input, training, mask):\n",
    "    token_embeds = self.token_embed(input)\n",
    "\n",
    "    # Generate position indices for a batch of input sequences.\n",
    "    num_pos = input.shape[0] * self.max_seq_len\n",
    "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
    "    pos_idx = np.reshape(pos_idx, input.shape)\n",
    "    pos_embeds = self.pos_embed(pos_idx)\n",
    "\n",
    "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
    "\n",
    "    # Run input through successive encoder blocks.\n",
    "    for block in self.blocks:\n",
    "      x, weights = block(x, training, mask)\n",
    "\n",
    "    return x, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of 3 sequences, each of length 10 (10 is also the \n",
    "# maximum sequence length in this case).\n",
    "seqs = np.random.randint(0, 10000, size=(3, 10))\n",
    "print(seqs.shape)\n",
    "print(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ids = np.resize(np.arange(seqs.shape[1]), seqs.shape[0] * seqs.shape[1])\n",
    "print(pos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ids = np.reshape(pos_ids, (3, 10))\n",
    "print(pos_ids.shape)\n",
    "print(pos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed(pos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = [\n",
    "    \"Where can I find a pizzeria?\",\n",
    "    \"Mass hysteria over listeria.\",\n",
    "    \"I ain't no circle back girl.\"\n",
    "]\n",
    "\n",
    "bpemb_en.encode(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = bpemb_en.encode_ids(input_batch)\n",
    "print(\"Vectorized inputs:\")\n",
    "input_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, padding=\"post\")\n",
    "print(\"Input to the encoder:\")\n",
    "print(padded_input_seqs.shape)\n",
    "print(padded_input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mask = tf.cast(tf.math.not_equal(padded_input_seqs, 0), tf.float32)\n",
    "print(\"Input:\")\n",
    "print(padded_input_seqs, '\\n')\n",
    "print(\"Encoder mask:\")\n",
    "print(enc_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mask = enc_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "enc_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_blocks = 6\n",
    "\n",
    "# d_model is the embedding dimension used throughout.\n",
    "d_model = 12\n",
    "\n",
    "num_heads = 3\n",
    "\n",
    "# Feed-forward network hidden dimension width.\n",
    "ffn_hidden_dim = 48\n",
    "\n",
    "src_vocab_size = bpemb_vocab_size\n",
    "max_input_seq_len = padded_input_seqs.shape[1]\n",
    "\n",
    "encoder = Encoder(\n",
    "    num_encoder_blocks,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    ffn_hidden_dim,\n",
    "    src_vocab_size,\n",
    "    max_input_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output, attn_weights = encoder(padded_input_seqs, training=True, \n",
    "                                       mask=enc_mask)\n",
    "print(f\"Encoder output {encoder_output.shape}:\")\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "    super(DecoderBlock, self).__init__()\n",
    "\n",
    "    self.mhsa1 = MultiHeadSelfAttention(d_model, num_heads)\n",
    "    self.mhsa2 = MultiHeadSelfAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  # Note the decoder block takes two masks. One for the first MHSA, another\n",
    "  # for the second MHSA.\n",
    "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
    "    mhsa_output1, attn_weights = self.mhsa1(target, target, target, decoder_mask)\n",
    "    mhsa_output1 = self.dropout1(mhsa_output1, training=training)\n",
    "    mhsa_output1 = self.layernorm1(mhsa_output1 + target)\n",
    "\n",
    "    mhsa_output2, attn_weights = self.mhsa2(mhsa_output1, encoder_output, \n",
    "                                            encoder_output, \n",
    "                                            memory_mask)\n",
    "    mhsa_output2 = self.dropout2(mhsa_output2, training=training)\n",
    "    mhsa_output2 = self.layernorm2(mhsa_output2 + mhsa_output1)\n",
    "\n",
    "    ffn_output = self.ffn(mhsa_output2)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    output = self.layernorm3(ffn_output + mhsa_output2)\n",
    "\n",
    "    return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
    "               max_seq_len, dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.max_seq_len = max_seq_len\n",
    "\n",
    "    self.token_embed = tf.keras.layers.Embedding(target_vocab_size, self.d_model)\n",
    "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    self.blocks = [DecoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_blocks)]\n",
    "\n",
    "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
    "    token_embeds = self.token_embed(target)\n",
    "\n",
    "    # Generate position indices.\n",
    "    num_pos = target.shape[0] * self.max_seq_len\n",
    "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
    "    pos_idx = np.reshape(pos_idx, target.shape)\n",
    "\n",
    "    pos_embeds = self.pos_embed(pos_idx)\n",
    "\n",
    "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      x, weights = block(encoder_output, x, training, decoder_mask, memory_mask)\n",
    "\n",
    "    return x, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made up values.\n",
    "target_input_seqs = [\n",
    "    [1, 652, 723, 123, 62],\n",
    "    [1, 25,  98, 129, 248, 215, 359, 249],\n",
    "    [1, 2369, 1259, 125, 486],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_target_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(target_input_seqs, padding=\"post\")\n",
    "print(\"Padded target inputs to the decoder:\")\n",
    "print(padded_target_input_seqs.shape)\n",
    "print(padded_target_input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_padding_mask = tf.cast(tf.math.not_equal(padded_target_input_seqs, 0), tf.float32)\n",
    "dec_padding_mask = dec_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "print(dec_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_input_seq_len = padded_target_input_seqs.shape[1]\n",
    "look_ahead_mask = tf.linalg.band_part(tf.ones((target_input_seq_len, \n",
    "                                               target_input_seq_len)), -1, 0)\n",
    "print(look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_mask = tf.minimum(dec_padding_mask, look_ahead_mask)\n",
    "print(\"The decoder mask:\")\n",
    "print(dec_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(6, 12, 3, 48, 10000, 8)\n",
    "decoder_output, _ = decoder(encoder_output, padded_target_input_seqs, \n",
    "                            True, dec_mask, enc_mask)\n",
    "print(f\"Decoder output {decoder_output.shape}:\")\n",
    "print(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, source_vocab_size,\n",
    "               target_vocab_size, max_input_len, max_target_len, dropout_rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_blocks, d_model, num_heads, hidden_dim, source_vocab_size, \n",
    "                           max_input_len, dropout_rate)\n",
    "    \n",
    "    self.decoder = Decoder(num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
    "                           max_target_len, dropout_rate)\n",
    "    \n",
    "    # The final dense layer to generate logits from the decoder output.\n",
    "    self.output_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, input_seqs, target_input_seqs, training, encoder_mask,\n",
    "           decoder_mask, memory_mask):\n",
    "    encoder_output, encoder_attn_weights = self.encoder(input_seqs, \n",
    "                                                        training, encoder_mask)\n",
    "\n",
    "    decoder_output, decoder_attn_weights = self.decoder(encoder_output, \n",
    "                                                        target_input_seqs, training,\n",
    "                                                        decoder_mask, memory_mask)\n",
    "\n",
    "    return self.output_layer(decoder_output), encoder_attn_weights, decoder_attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_blocks = 6,\n",
    "    d_model = 12,\n",
    "    num_heads = 3,\n",
    "    hidden_dim = 48,\n",
    "    source_vocab_size = bpemb_vocab_size,\n",
    "    target_vocab_size = 7000, # made-up target vocab size.\n",
    "    max_input_len = padded_input_seqs.shape[1],\n",
    "    max_target_len = padded_target_input_seqs.shape[1])\n",
    "\n",
    "transformer_output, _, _ = transformer(padded_input_seqs, \n",
    "                                       padded_target_input_seqs, True, \n",
    "                                       enc_mask, dec_mask, memory_mask=enc_mask)\n",
    "print(f\"Transformer output {transformer_output.shape}:\")\n",
    "print(transformer_output) # If training, we would use this output to calculate losses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
